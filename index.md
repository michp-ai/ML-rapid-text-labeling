# Faster More Accurate Text Labeling
## How we Built a Web App using Data Science to Vastly Improved Speed and Accuracy in Text Labeling

### <span style="color:blue">Capstone Project for Master of Applied Data Science</span>
### <span style="color:blue">University of Michigan School of Information</span>

### Contributors:
* [https://github.com/Tak-Man](https://github.com/Tak-Man) 
* [https://github.com/michp-ai](https://github.com/michp-ai)

December 2021

## ML Rapid Text Labeling - High Level Scenario
The amount of natural language text that is stored is growing rapidly and there is an ever-increasing demand to extract information from that text. Many supervised learning applications exist for Natural Language Processing (NLP). Supervised learning requires labels to be attached to the text. Many texts do not have labels currently and there are many cases where it would be beneficial to add labels to text. Labeling can be slow, laborious, and prone to error. There are many questions about labeling such as, how many labels is enough? Is it better to aim for more labels, or to aim for fewer labels of higher quality? What is an efficient way of labeling texts? In this Capstone project we build a web app that illustrates a user-friendly way of doing rapid text labeling. It gives a real-time indicator to the labeling user that can act as a reasonable guide about how many labels are enough. The web app implements multiple Data Science and Machine Learning techniques to enable the labeling user to progress through the labeling of a text corpus making decisions along the way about the trade-off between accuracy and speed. The final project submission consists of two repositories plus this repo hosting this blog write-up of the project. The code repos are:
1. [https://github.com/Tak-Man/ML-rapid-text-labeling-app](https://github.com/Tak-Man/ML-rapid-text-labeling-app) This is the web app itself
2. [https://github.com/Tak-Man/ML-rapid-text-labeling](https://github.com/Tak-Man/ML-rapid-text-labeling) This repo is used to analyse the dataset selected for this project and evaluate the performance of the web-app in terms of speed and accuracy.

### Dataset
The web app is designed to allow unlabeled datasets to be labeled. However, in the project itself, we used a labeled dataset but then in the app treated it as unlabeled. We did this because it allowed us to measure certain benchmarks in terms of accuracy and to see how well the web app performed against those benchmarks. If we had started with a completely unlabeled dataset, we would not have been able to have that baseline comparison. Using a pre-labeled dataset also had some further advantages in terms of evaluating the product. These will be explained later in more detail. At a high level though we evaluated the product partially through manually interacting with the web app, partly through web automation to simulate multiple potential paths that a user could reasonably take given the dataset and the functionality available in the web app, and partly by doing NLP on the underlying dataset directly to establish the benchmarks themselves.

The main dataset used in this project is the [Disaster tweets dataset](https://crisisnlp.qcri.org/humaid_dataset.html#). This dataset was pre-approved for the Capstone project and meant that the project could focus on steps 1, 3 and 4 of the Data Science Project lifecycle namely Project Design, Analysis and Modeling and finally deployment and presentation of results. Step 2 data collection and cleaning was effectively done by the providers of this dataset. In the Milestone projects I spent a lot of time and effort on data collection through automated web-scraping and for this project it was more beneficial to focus on the other areas mentioned which was made possible by the use of a pre-existing dataset. The dataset contains fifty-three thousand labeled training examples. Another advantage of this dataset is that it contains 2 separate types of labels. There is an event type which is one of "Earthquake", "Fire", "Flood" or "Hurricane". This is the label type used in the web app. These event type labels along with their value counts are shown in the figure below. We see that the labels are unbalanced with over 31,000 of the 53,000 total belonging to one of the four classes.

![labels_df](https://user-images.githubusercontent.com/48130648/144719551-6904a108-089b-4c66-a4c3-e55d9c2a7fc6.JPG)

There is also a class type which classifies the event not so much in terms of the type of disaster but more in terms of the consequences of the event and human responses. The figure below illustrates the class type of labels with the dataset along with their value counts.

![alt_labels_df](https://user-images.githubusercontent.com/48130648/144719083-23410271-6b13-401b-959a-9424f6ed2546.JPG)

### Functionality of the Web App
The web app contains a wide range of functions. It allows users to (1) label a single tweet at a time from a list of tweets in the dataset, (2) label groups of tweets based on the cosine similarity of their text, (3) label groups of tweets based on predictions made by the SGD Classifier, (4) label tweets by searching for text with include and exclude terms, (5) label the tweets that the SGD Classifier model is least confident in predicting, (6) label all remaining unlabeled tweets, and (7) to download the generated labels plus the SGD Classifier model trained on those labels.

This demo video introduces the key functionality:

<video src="https://user-images.githubusercontent.com/48130648/146453588-6ce8dbb9-14d3-46e9-9dd2-abc9f4b70380.mp4" controls="controls" style="max-width: 730px;">
</video>

The web app itself is deployed [here](http://ml-rapid-text-labeling-app.herokuapp.com/). This is a working version of what is described in the blog containing all the functionality just described. The current deployment of the app (at the time of writing) is a demo to show how machine-learning can be used to label texts rapidly. Further development is planned. Currently the app can accommodate single user use.

## How we Built the Web App
### Development with Flask
The problem that this project sought to address was the long time it takes for a person to label a large corpus of texts in a useful manner. To explain this consider the following scenario that occurred in the working life of one of the project team members. The company that the team member worked for was receiving a large volume of text feedback posted to a website. Most of these pieces of feedback were not addressed in a meaningful manner by the company because there was not enough available staff to read through all the texts and then assign (categorize) them to the correct department to be addressed. To try to ameliorate this situation a classifier (machine-learning model) was built that could assign a label to the texts. To build this classifier, thousand of texts were read through and an appropriate label (that tied back to an internal department) was assigned to the text. These labeled texts were then used to train a machine learning classifier that could then give an appropriate label to future texts that were received and thus speed up the routing of the customer feedback. After some retrospection and the sharing of experiences between the project teammates, it was realized that the manual labeling of texts was the slowest part of this process and if that could be improved then these types of scenarios (categorizing a large corpus of texts) do not have to be so slow. Also, some of the concepts covered during the MADS program inspired this project – these were courses like Applied Natural Language Processing, Data Mining, Supervised Learning, Unsupervised Learning, and Search and Recommender Systems.

It was felt that a web app would, at the very least, serve as a proof of concept for the ideas that we had in mind. Also, one member of the project team previously had experience with the Flask framework, and so development proceeded with Flask.

The Flask framework, itself, was fairly easy to use and the most challenging part was to get the framework to deliver what we had envisioned. For example, we wanted the user to be able to constantly see which texts were the most similar to a selected text. This is not a Flask-specific problem, but a problem with application development in general – there are many ways to implement a particular feature, but in our case, we didn’t want calculation of the similar texts to slow the user experience down. There were many examples like this, where a performant version of the feature had to be developed through many iterations of the feature. In the case of displaying the similar texts, we choose to pre-compute the vectorized text before labeling begins and then only compute the similarities for the selected text (this might have resulted in more computation overall – instead of computing the entire similarity matrix once, upfront,  and then just looking up texts in it) but we found by computing the similarity in pieces, the user experienced few delays.

### Deployment with Heroku
Heroku was chosen because it was believed that the web app (as developed in Flask) could easily be deployed to it. Most of the development of the web app was done using a local server (on P.C. development). Heroku was also chosen because it had a free tier – we hoped not to incur a cost for the app’s deployment because we primarily wanted to prove that the concept works; not to host a high traffic app.

The initial deployment to Heroku brought forth a major problem – the application’s first screen would load but then any clicks on the buttons caused the application to crash. A lot of frantic internet searches and combing through the Heroku logs indicated that the problem was caused by the primary data structure used in developing the app, up to that point – list. The information generated during the app’s usage was passed using lists. It should be noted that, although a definitive explanation of why the local application development was successful using lists, but the deployment to Heroku caused an error, was never found, by removing the lists the app was able to run successfully in Heroku. To test that this would indeed solve the problem, we proceeded to replace the lists with data stored in a SQLite3 database (hosted on Heroku) and observed that the errors proceeded to disappear. In the end, a large portion of the app was rewritten in a short amount of time. The performance of the re-written app appears to be as good as before.

### How did we think about Speed and Accuracy?
We thought about speed mainly from the user's point of view. We thought about it as the number of labeling actions, and the length of time each labeling action took. This meant that we had to use Data Science techniques that ran fast enough that someone using a website would find acceptable. There is always a trade-off between speed and accuracy in any labeling process. Although we wanted the web app to be fast, we also wanted to achieve reasonable accuracy. This is not the same as optimal accuracy that could be achieved by individually labeling each of the 53,000 examples and then attempting a Kaggle style optimisation of the accuracy against the test set by running multiple different sophisticated models and ensembling the results together. In our app we gave a lot of control to the user about how they want to manage the trade-off between speed and accuracy. If they want to optimize on accuracy they can, it will just take a little longer. If they need to get it done as soon as possible the app can help them do that too, it just might be a little less accurate. Because the app offered that flexibility, we needed a model that could perform reasonably well on both speed and accuracy. For that we chose the SGDClassifier implementation in scikit-learn. The More Details on Benchmarking section at the end explains how we made that choice. We were looking for a reasonable level of accuracy that could be acceptable in a real-world scenario. We also used various techniques to optimize what could be achieved at any point in the speed vs accuracy trade-off.

### What we learned through using the App Ourselves
Using the app ourselves a few things became obvious. The functionality to label single texts worked in a timely fashion and could allow labeling an entire corpus, however, it would be extremely slow. As in the example of an initial labeling of emails, only in the case where extreme accuracy was needed would it make sense to label 53,000 individually. The functionality to select a single text then locate similar texts by cosine similarity also worked well and enabled a batch labeling of those other texts with the same label. It also allowed each of the batch of tweets to be inspected before the user decided to batch label, and it allowed the user to adjust the size of the batch. Perhaps the user is happy with the top 10 most similar texts that they do share the same label but in the top 50 most similar they may notice a few that should not share the same label so they can choose to just label the 10 most similar. This will still be much quicker than individual labeling. The user doesn’t have to manually check the similar texts, but the option is available. Once some labels of all types are available the functionality to learn from existing labels to generate new labels is available. Just like the similar texts functionality the user can inspect the texts that the app is proposing to label and can control the size of the batch to achieve their desired trade-off between speed and accuracy. The ability to search for texts and have exclude words in the search worked well. This was not constrained to the size of the batch so in some cases we saw that in one action we were able to label more than 30,000 of the 50,000 texts just by finding texts that matched one of the keywords. The processing time to label 30,000 tweets was a little longer than labeling 1 or 100 at a time but it was not unacceptably slow. Another tool that worked well was the difficult texts section. This highlighted the texts that the supervised model was least confident about labeling. This allowed the user to home in on those ones and provide the true label. Through using the app, we also realised that a save function would be very useful and this was implemented around half way through the project.

### Feedback we got from User Demos
We conducted four user demos with members of the instructional team. The first was with Professor Mei. We generally got very positive feedback from this demo and a suggestion that this could be useful even for NLP work being done at UMSI. We did this first demo early in the project and the feature to label based on search (with exclusion words and phrases) was developed in response to that initial conversation.

We had a second opportunity to get user feedback particularly focused around UX which was very helpful. As a result, an alternative more user-friendly layout was developed as an option. This is now Layout 2 in the web app. Layout 1 remains as an option. We agree that Layout 2 is more intuitive for users and does not place as much visual information on the screen at the same time. This would certainly be more approachable for users early in their experience of using the app. We retained our existing layout as an option as well, since having got used to it quickly we felt that long-term users of the app might prefer not having to expand and contract sections but might prefer to have all the information on the same screen even if it would look cluttered to someone seeing it for the first time. For user experience, as in the speed vs accuracy trade-off and the choice of Data Science techniques to be used in the labeling we designed the app so that the user has maximum choice about their preferred approach.

In the third presentation we talked about some further ML considerations about this approach and the importance of indicating to the user when is a good time to stop. The importance of knowing when to stop was also repeated when we met again with Professor Mei for our mid-way checkpoint.

It was good to get these other perspectives that touched on points we had not considered. For me personally the UX session was possibly the most useful because it was the one where we were both had least knowledge and experience.

Overall, the feedback we got was very positive and the users were all excited about what we had built which was encouraging. We also got similar feedback from our video standup presentation of the product. The feedback was so encouraging that we got various suggestions for how we could take this project forward. All we can say to that is watch this space and you never know…

### What we Learned from Automated Evaluation

The manual testing of the app we did plus the user demos gave us a good sense of how the app could perform in terms of user experience and speed of labeling. But we needed a new approach to understand how it performed in terms of speed vs accuracy. Because we wanted to check how it was performing at many different points in the labeling process, and we wanted to explore different pathways through using the app, we realised an automated solution would be required. We chose Selenium for this which is python library (versions also exist for Java and other languages) designed for automated testing of websites. Using the Selenium library in python we were able to explore many different ways of using the app and how they performed in terms of speed vs accuracy. Unlike a human user Selenium could continue labeling for many hours at a time to get through thousands of labeling actions without needing a break and with the ability to exactly recreate all the same steps. After each new labeling action we got Selenium to invoke the export records feature in the app. As well as exporting the labels generated so far, this feature also exported the SGDClassifier model generated by the app. We used that model outside the app to test how accurately our model could predict against the test set with the labels provided. We recorded our results at each step and used them to conduct analysis about model performance. In the short video below you can see Selenium in action testing the app. This shows a few steps in the early stages of a long experiment. You can see in the video that each time a label is added the app clicks the "Generated Difficult Texts" button which updated the Overall Quality Score. Selenium then captures that score before downloading the current labels and model and using the model to predict against the test set. The score on the test set is recorded to a pandas dataframe. Initially we tried using a heuristic in the python selenium evaluation code to label individual texts but later we decided where we were labeling texts one-at-a-time we could look up the label that was given by a human labeler in the original labeling of the dataset. The app could not see the original label, but the selenium code, running in a different conda environment could see it. Remember that although the app is designed for unlabeled corpuses, by using a labeled corpus we were able to measure the accuracy that could be achieved by models using the app. We ran many experiments like the one shown in the video and a few ran for over 24 hours! Given that Selenium can likely keep up a sustained pace of labeling well beyond what a human labeler could sustain, this puts in context the importance of the massive time-savings in the labeling process achieved by the app that will be described below.

<video src="https://user-images.githubusercontent.com/48130648/146455916-5dba8fb5-b79d-4d02-afe9-3522c2ee91bb.mp4" controls="controls" style="max-width: 730px;">
</video>

We tried many different experiments in this way on all the key functionality in the app but in this blog we will focus on the most important results where there was a clear and substantial benefit to the labeling user.

### Automated Evaluation - Increases in Accuracy
Our analysis showed that the highest accuracy against the test set was achieved by a model learning from a situation where every training example was labeled individually. In terms of the trade-off between speed and accuracy this is the extreme end of the spectrum at maximal accuracy. Our purpose was not to improve that accuracy through some kind of Kaggle style optimisation challenge. However, we did look for scenarios where the app led to increased accuracy in certain conditions at certain points of the process versus a monotonic procession through all the examples labeling one at a time. One intuition was that the model might learn quicker where it is forced to train on the hardest to predict labels part of the time rather than random labels. The Difficult texts functionality in the app allows this to be done. First, we set up a baseline where we labeled only on texts selected in a random order via the All Texts functionality. (The randomization is done before the data is presented to the user for the first time). We worked through the data in sequence and labeled each tweet with the exact label that the original annotation did. At the end of every 50 tweets, we simply moved to the next page and repeated the process. Next, we labeled the first 50 tweets in exactly the same way. But before moving to the next page on All Texts, we labeled the 10 most difficult tweets from the Difficult Texts function. Then we did another 50 tweets via All Texts and 10 more from Difficult Texts and so on until we reached the end of the experiment. Finally, we repeated the process but doing 20 difficult texts each time rather than 10. This improved accuracy for no loss of speed versus individual labeling as the following charts illustrate.

![eda_difficult](https://user-images.githubusercontent.com/48130648/144916997-4cd3da47-dfe3-47af-8b94-8b414eac4cf0.png)

The first chart shows the full extent of the experiment. It is clear that where the difficult texts functionality is used it leads to consistently improved accuracy versus simply working through the texts in order.

If we zoom in to the portion of the experiment where some labels are in place and accuracy has exceeded 80%, we can see more clearly that using 20 difficult texts after each 50 texts labeled via all texts leads to a further slight increase in accuracy versus labeling 10 difficult texts after each 50. It also shows that both are considerably ahead of the accuracy achieved without difficult texts being used.

![eda_difficult_closeup](https://user-images.githubusercontent.com/48130648/144917295-7f0b4d53-de4f-483c-a2e9-19151cbba882.png)

It may be a little abstract to think of accuracy being 3% or 4% higher so we can visualise this another way which makes the improvement more concrete for a labeling user. If we instead visualize how many labels are required via each approach to reach a threshold accuracy level, we can see that our approach could save a labeling user a lot of time.

![eda_difficult_threshold_0 9](https://user-images.githubusercontent.com/48130648/144917696-e0a73ef3-1bb8-473f-87b9-9736e5925674.png)

First to be clear on the process. The model in the app is trained on the labels provided in the labeling experiment then the same model is used to predict against the test set. The first time the model reaches 90% accuracy on the test set the number of training labels required to reach that threshold is recorded. We can see a vast improvement in time taken to reach the threshold where the Difficult Texts functionality is used. In fact, it takes more than twice as long to reach the threshold without using Difficult Texts as it does using the 50 All Texts 20 Difficult Texts labeling pattern. We were very happy with this outcome as it shows that the app doesn't just lead to improvements in speed, but it can improve accuracy in meaningful ways that correspond to realistic ways that the app could be used.

Having seen these encouraging results, we decided to explore further by moving towards using more difficult labels and running longer experiments. We tried labelling alternately 50 tweets from the All Texts list followed by 50 tweets from the Difficult texts list. Then we tried initially labeling 50 tweets from the All Texts list so that the model had an initial starting point then just using Difficult labels for the rest of the time. The results are shown below.

![speed_accuracy_95pct_threshold](https://user-images.githubusercontent.com/48130648/145678440-f1108417-0db6-4456-a24e-2b2eabbb40fe.png)

Both of these approaches led to massive time-savings in terms of labeling for achieving same level of accuracy when a model was trained on those labels and used to predict against the test set. The time saving for alternately labeling 50 tweets from the All Texts list then 50 tweets the model was least confident about was 70% compared to just labeling from the All Texts list. The time saving for labeling 50 tweets from All Texts then just using the Difficult Texts option from then onwards was even greater at 82%. The option that the app provides for smart selection of what to label next via Difficult Texts enables massive time-savings in terms of user labeling effort. This is where each approach achieves the same level of accuracy when a model was trained on those labels and used to predict against the test set. This experiment had the added benefit that the accuracy on the test set of 0.95 was reasonably close to the optimal accuracy of 0.969 that is achieved when all 53,000 rows of training data and their associated training labels are used to train the exact same model. In many real-world situations this level of accuracy might be acceptable and in those where accuracy is important the app allows the user to continue labeling more examples to get further accuracy improvements.

### Automated Evaluation - Indicators for When to Stop Labeling
The results just described are great, but, in the real world where the user doesn't have a test set to benchmark against, how will they know when it makes sense to stop labeling? This is a really important question about the usability of the app. Using the same experiments described above we plotted accuracy against the test set (which would not be available on an unlabeled corpus) against the Overall Quality Score in the app. We first did this for texts labeled via All Texts using data from the full experiment.

![overall_quality_all_10000](https://user-images.githubusercontent.com/48130648/145680316-14f75a79-f193-49c9-970c-586c2f8db004.png)

We can see that overall quality does track accuracy pretty closely and since the user doesn't have access to accuracy directly, it could be a useful proxy for when to stop. If we compare those results to labeling via the Difficult Texts approach, we see again that the overall quality score could serve as a reasonable proxy of accuracy and could be used to allow the user to make decisions about when to stop once the number of labels provided gets past a few hundred.

![overall_quality_difficult_texts](https://user-images.githubusercontent.com/48130648/145680417-b8ea1156-5c81-4cea-9292-82fa328821b1.png)

Another thing we can see from these last two charts is that both Overall Quality Score and Accuracy are less volatile using the Difficult Texts approach. The web app also enables this as another desirable outcome for the user who is labeling texts and wants useful information to help them decide when to stop labeling. 

Overall, this is great news for the app. Not only does using the Difficult Texts feature to select the best tweets to label next for rapidly increasing accuracy, but the app also gives the user an effective signal in terms of the Overall Quality Score that helps them to decide when to stop labeling.

## Considerations for Future Work

The following are thoughts about future work that might be done on this app:
* Even though this application was developed primarily as a web application, it might be useful to some users to have a version of the application that is deployed locally. 
  * This would keep the user data from being on the internet which might be useful if the user makes use of data that should not be on the internet. 
  * A local version of the app might give better performance because it would rely on the user-provided computation resources instead of the server (or servers) providing the computational power for the application.
* Keeping the app as a web app would have benefits too. These could be things like:
  * Future development of the app would be easier to deploy because the users would be accessing the app from a central place (the deployed version of the app on the internet). 
  * The users could benefit from not having to consider what computational resources would be needed to run the application.
* An option to show only unlabeled tweets in All Texts
* An option to allow multiple save states not just one. (This is a constraint of the current free hosting option, that could be removed if we went towards paid hosting.)
* Potentially add an option for the Label All Texts button to use slower and more powerful models. Most of the time the user is interacting with the app speed is important, but some users may be willing to wait longer to get the final labels. For example, a user might start labeling all texts before lunch and be willing to wait the length of a lunchbreak to get the final output, or they might be willing to wait overnight if it would lead to more accurate labels. That could allow the implementation neural networks and pre-trained embeddings which would likely improve accuracy. Two reasons we did not do this initially were speed and the constraints of free hosting.
* A visual display of the progress of the Overall Quality Score.
* Evaluate with other text corpuses.
* The underlying assumption used in the application is that the user is responsible for the usage of the results of this application. This means that if the user chooses to label a certain text one way but another user chooses to label it another way, then the user is ultimately responsible for the results. Also, it would be unfeasible for the creators of the application to curate and control all of the application’s usage. This might bring up ethical concerns that should be considered. Future work thus could include considerations of the app’s usage that could have ethical implications. Currently, two approaches are being considered:
  * Provide a conspicuous disclaimer upfront, possible before even allowing the user to access the application, check if they agree to some terms and conditions of use of the application.
  * Emphasize the usage of the ‘Difficult Texts’ section. This section shows how the trained machine learning model will treat certain texts (probabilities of class membership). Currently, the ‘Difficult Texts’ section is a table in the web app, but in the future, it might be augmented (or replaced) by a dialogue with the user. This could look like this:
    * Several texts are ambiguous because they are likely to be labeled ‘Flood and ‘Other. Here is an example of such a text :
      * <i>‘ὠBὠBὠBὠBὠB you should try this when you have the munchies ὡC @ Screaming Carrots’</i>, what should this be labeled as?

By directing the user to such ‘difficult’ texts we hope to make the text labeling process clear to the user and thus increase their confidence with the application, as well as improve the quality of labels assigned.

## Conclusion
There is always a trade-off between speed and accuracy in labeling a previously unlabeled text corpus. The wide range of functionality in the web app we developed allows the user a lot of control over how they approach that speed-accuracy trade-off. The web app also allows an effective user experience. The user of the Difficult Texts function in the web app could be described as the ace feature that means acceptable accuracy can be achieved with much less labeling effort. Meanwhile, the Overall Quality Score provides the user with a clear indicator that is a good proxy for how accurately a model would perform based on the labels provided so far in the labeling process. By starting early, we were able to achieve a lot in the two months of our Capstone project. This is a good foundation for further developments of functionality and Applications of Data Science specifically in Natural Language Processing.


## More Details on Benchmarking
This section contains more details about the work we did on benchmarking and selecting which machine learning model to use in the app. You can think of it as a kind of appendix. The TLDR is that the SGDClassifier model we originally selected turns out to be a great choice for the compromise it achieves between accuracy and speed. The benchmarking did have practical value to the app though because it made us see the benefit of increasing the number of max features from the vectorizer which lead to big improvements in accuracy with minimal cost in terms of speed. We implemented this change in the app and the Selenium experiments described above benefited from this higher accuracy.

As a starting point we trained the model on different sized subsets of the training data and used the trained model to predict against the test set provided with the data. We then plotted a curve to see the accuracy achieved against the test set with differing amounts of training data. We repeated this for three different fast machine learning models available in scikit-learn. Another important variable that impacts speed and accuracy is the maximum number of features output by the vectorizer. We experimented setting this to 100, 200, 300, 500, 800 and leaving it blank i.e., to take the full output of the vectorizer without constraining the number of features. We produced several charts of our benchmark runs. These visualizations helped us to compare the results in terms of speed and accuracy across the various settings of our grid search on the parameters of model type, max number of vectorizer features, and number of training examples. 

We conducted analysis of the dataset with both of the available type of labels provided in the initial dataset. This is because we recognise that there may be more than one way of labeling a text corpus and depending on the type, balance and number of labels required, the speed vs accuracy trade-off could shift, and the number of labels required before accuracy plateaus could also vary. 

The figure below shows the results of our benchmark analysis on 5 different types of base models plus a stacking classifier that combined all five and ran an SGDClassifier on the output of those to predict against the test set. The x axis illustrates how much of the training data was used in each case. The colors indicate the number of vectorizer features used (null means there was no limit and all vectorizer features were used). The highest accuracy achieved predicting on the test set in any of these tests was by the stacking classifier trained on the entire training set. The accuracy in this case was 0.9747. The accuracy on the test set for the LinearSVC model trained on all the training data was 0.9743. This might make a big difference in a Kaggle competition but for most practical purposes the uplift from stacking may not be worth the additional technical complexity and run time. 

![main_target_baseline_accuracy](https://user-images.githubusercontent.com/48130648/144724590-f29feb6a-1812-4a70-8dae-e99033272505.png)

From this chart we learnt quite a few things. Firstly, MultinomialNB models were not competitive on accuracy and needed a lot more training data to learn from the more complex input provided when the number of vectorizer input features was not constrained to a maximum value. On all other models we see that more vectorizer features led to much greater accuracy pretty rapidly. Using 100 vectorizer features the accuracy was well below what could be achieved by 200 and there were improvements on all models for each additional increase from 200 to 300, 300 to 500 and 500 to 800. By the time 800 features were used most of the gap in performance between 100 features and having no limit was covered. Accuracy was more volatile on the Perceptron and PassiveAggressiveClassifer whilst the highest accuracy was achieved with LinearSVC closely followed by SGDClassifier. 

If we look just at the results on the first 3000 training examples, we learn some additional things. The accuracy with stacking is 0.9550 at this point but with the SGDClassifier it is 0.9489 which is quite a bit lower. With LinearSVC the best accuracy of 0.9495 is achieved with 800 max features rather than the full output of the vectorizer showing that LinearSVC needs a few thousand more training examples to fully benefit from the increased complexity of a model using full vectorizer output. For Multinomial we can see that initially it does better with the fewest number of vectorizer features. 

![main_target_baseline_accuracy_early](https://user-images.githubusercontent.com/48130648/144741746-ac41ed87-51f6-42cb-b225-3a82a76fedce.png)

The following chart illustrates the accuracy versus run-time of the various models split out by the maximum number of vectorizer features. From this we can again see that MultinomialNB models were not competitive on accuracy and LinearSVC models were relatively not competitive on speed.

![main_target_speed_accuracy](https://user-images.githubusercontent.com/48130648/144724744-0026f17e-252e-4ebd-8b14-11661b14d489.png)

We initially chose SGDClassifier as the model for the app and based on this analysis we did not change that although we did increase the number of max features in the web app from the vectorizer from 100 to 800 to increase accuracy and we set n_jobs to -1 to increase speed. Our analysis showed that SGD achieved a good trade-off between speed and accuracy.

Speed on the stacking classifier is shown on the following chart and is clearly quite a bit slower than the individual charts. This is where SGD is used as the stacking model. Where LinearSVC was used as the stacking model in some cases we saw run times of 200+ seconds which is clearly an unacceptable wait time given the alternatives available with single models or the much faster SGD stacking. Again, we see that using all features from the vectorizer takes a lot longer at 7.5 seconds training on all the data vs 2.7 training on all the data and limiting the vectorizer output to the top 800 features. We also notice an anomaly that in some cases the longest run-time is for the fewest number of training examples. For example, using 800 vectorizer, training on 250 examples took 3.15 seconds compared to 2.7 training on all 53,000 records of training data.

![main_target_stacking_speed_accuracy](https://user-images.githubusercontent.com/48130648/144742963-b989e838-8575-4652-8496-820189aaa3d6.png)

Since there were two different types of labels available for this dataset, we ran some of the same analysis but using the other label set. The results illustrate some important differences. The most obvious difference is that the highest accuracy achieved is much lower at under 0.75 versus over 0.97 on the event type labels. We can also see that more training examples are needed before the improvement in accuracy starts to flatten compared to the event type target. For the class labels accuracy was noticeably improving even after all the training data was being used.

![harder_target_baseline_accuracy](https://user-images.githubusercontent.com/48130648/144743926-17133368-3aad-4261-b2dd-ba577008e374.png)

In general, we found that the models took a bit longer to learn on the harder to classify class label and this decreased speed was especially noticeable on the LinearSVC model. For the full amount of training data and all vectorizer features this model took nearly 4 seconds to run whilst all the other models ran in under 1.5 seconds. This is a big difference to a user of a website. The chart also shows the benefit of not using all vectorizer features but instead just taking the top 800. Doing this LinearSVC ran in under 2 seconds and the other models tended to run in less than half a second which again would be noticeable to the user of a website especially for a process that could be repeated hundreds or even thousands of times.

![harder_target_speed_accuracy](https://user-images.githubusercontent.com/48130648/144743931-8ec356b0-eef3-4853-8811-02372bd7c30d.png)

In summary what we learned from this benchmark analysis is that the SGD Classifier achieved a good trade-off between accuracy and speed since it was more accurate than most single models and much faster than LinearSVC. Stacking in some cases achieved the highest accuracy although not always and the increased accuracy came with an expense in terms of speed. We also learned that even using the same dataset with different label types, the point at which accuracy predicting against a test set begins to plateau can vary quite a lot. This is important because it illustrates that there is no fixed point where the marginal gains of increased labeling tail off and it makes sense to stop labeling. This is why it is very useful to give the user of the app who is labeling a previously unlabeled corpus an indicator to help them decide when to stop labeling. As we showed in the section on Automated Evaluation above, the Overall Quality Score in the app provides an effective signal to the user to help them make this decision.






